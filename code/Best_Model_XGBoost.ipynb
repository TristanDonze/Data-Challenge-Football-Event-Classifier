{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from Dataset import get_data\n",
    "import gensim.downloader as api\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"glove-twitter-200\"\n",
    "embedding_dim = 200\n",
    "embeddings_model = api.load(embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'rt', '', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "def tokenize_and_lemmatize(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_tweets(tweets):\n",
    "    cleaned = [clean_tweet(tweet) for tweet in tweets]\n",
    "    tokenized = [tokenize_and_lemmatize(tweet) for tweet in cleaned]\n",
    "    return tokenized\n",
    "\n",
    "def get_embedding(tokens, embeddings_model, embedding_dim):\n",
    "    valid_embeddings = [embeddings_model[token] for token in tokens if token in embeddings_model]\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "def create_embeddings(data, embeddings_model, embedding_dim):\n",
    "    embeddings = []\n",
    "    for tweets in data['Tweets']:\n",
    "        tokenized_tweets = preprocess_tweets(tweets)\n",
    "        all_tokens = [token for tweet in tokenized_tweets for token in tweet]\n",
    "        embedding = get_embedding(all_tokens, embeddings_model, embedding_dim)\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matchs, all_matchs_cleaned, all_matchs_eval, all_matchs_cleaned_eval = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for match, df in all_matchs_cleaned.items():\n",
    "    for id, group in df.groupby('ID'):\n",
    "        tweets = group['Tweet'].tolist()\n",
    "        event_type = group['EventType'].iloc[0]\n",
    "        data_list.append({\n",
    "            'ID': id,\n",
    "            'EventType': event_type,\n",
    "            'Tweets': tweets\n",
    "        })\n",
    "\n",
    "data = pd.DataFrame(data_list)\n",
    "data[['MatchID', 'PeriodID']] = data['ID'].str.split('_', expand=True)\n",
    "data['MatchID'] = data['MatchID'].astype(int)\n",
    "data['PeriodID'] = data['PeriodID'].astype(int)\n",
    "data = data.sort_values(by=['MatchID', 'PeriodID']).reset_index(drop=True)\n",
    "data = data.drop(columns=['MatchID', 'PeriodID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_embeddings(data, embeddings_model, embedding_dim)\n",
    "y = data['EventType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Embeddings.npy', X)\n",
    "np.save('Labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.load('Embeddings.npy')\n",
    "# y = np.load('Labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy XGBoost: {accuracy_xgb:.4f}')\n",
    "print('Classification Report XGBoost:')\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_list = []\n",
    "for match, df in all_matchs_cleaned_eval.items():\n",
    "    for id, group in df.groupby('ID'):\n",
    "        tweets = group['Tweet'].tolist()\n",
    "        data_eval_list.append({\n",
    "            'ID': id,\n",
    "            'Tweets': tweets\n",
    "        })\n",
    "\n",
    "data_eval = pd.DataFrame(data_eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval_embeddings = create_embeddings(data_eval, embeddings_model, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Embeddings_eval.npy', X_eval_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_eval_embeddings = np.load('Embeddings_eval.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval = xgb_clf.predict(X_eval_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'ID': data_eval['ID'],\n",
    "    'EventType': y_pred_eval\n",
    "})\n",
    "\n",
    "submission[['num1', 'num2']] = submission['ID'].str.split('_', expand=True).astype(int)\n",
    "submission = submission.sort_values(by=['num1', 'num2']).drop(columns=['num1', 'num2'])\n",
    "\n",
    "submission.to_csv(f'Result', index=False)\n",
    "print(\"Prediction has been saved in 'Result.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
