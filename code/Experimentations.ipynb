{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model contains experimentations with Embeddings and Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from Dataset import get_data\n",
    "import gensim.downloader as api\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import math\n",
    "import matplotlib.dates as mdates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matchs, all_matchs_cleaned, all_matchs_eval, all_matchs_cleaned_eval = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison of the evolution of the number of Tweets over time between raw and cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_minute_original = {}\n",
    "\n",
    "for fileName, df in all_matchs.items():\n",
    "    \n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    \n",
    "    aggregated_data = (\n",
    "        df.set_index('Timestamp')\n",
    "        .resample('min')\n",
    "        .size()\n",
    "        .reset_index(name='tweet_count_original')\n",
    "    )\n",
    "    tweets_per_minute_original[fileName] = aggregated_data\n",
    "\n",
    "tweets_per_minute_cleaned = {}\n",
    "\n",
    "for file_name, df in all_matchs_cleaned.items():\n",
    "    \n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    \n",
    "    aggregated_data = (\n",
    "        df.set_index('Timestamp')\n",
    "        .resample('min')\n",
    "        .size()\n",
    "        .reset_index(name='tweet_count_cleaned')\n",
    "    )\n",
    "    tweets_per_minute_cleaned[file_name] = aggregated_data\n",
    "\n",
    "\n",
    "if not tweets_per_minute_original or not tweets_per_minute_cleaned:\n",
    "    raise ValueError(\"Aucune donnée agrégée trouvée pour les fichiers CSV.\")\n",
    "\n",
    "\n",
    "num_matches = len(tweets_per_minute_original)\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(num_matches / num_cols)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False, sharey=False)\n",
    "\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for idx, (fileName, original_data) in enumerate(tweets_per_minute_original.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    \n",
    "    cleaned_key = f\"{fileName}_clean\"\n",
    "    cleaned_data = tweets_per_minute_cleaned.get(cleaned_key)\n",
    "    \n",
    "    if cleaned_data is not None:\n",
    "        \n",
    "        ax.plot(\n",
    "            original_data['Timestamp'],\n",
    "            original_data['tweet_count_original'],\n",
    "            color='blue',\n",
    "            label='Original'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ax.plot(\n",
    "            cleaned_data['Timestamp'],\n",
    "            cleaned_data['tweet_count_cleaned'],\n",
    "            color='red',\n",
    "            label='Cleaned'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ax.set_title(f\"Match: {fileName}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Time\", fontsize=8)\n",
    "        ax.set_ylabel(\"Number of Tweets\", fontsize=8)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        \n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=6)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=6)\n",
    "        \n",
    "        \n",
    "        ax.legend(fontsize=6)\n",
    "    else:\n",
    "        \n",
    "        ax.plot(\n",
    "            original_data['Timestamp'],\n",
    "            original_data['tweet_count_original'],\n",
    "            color='blue',\n",
    "            label='Original'\n",
    "        )\n",
    "        ax.set_title(f\"Match: {fileName} (No Cleaned Data)\", fontsize=10)\n",
    "        ax.set_xlabel(\"Time\", fontsize=8)\n",
    "        ax.set_ylabel(\"Number of Tweets\", fontsize=8)\n",
    "        ax.grid(True)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=6)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=6)\n",
    "        ax.legend(fontsize=6)\n",
    "\n",
    "\n",
    "for j in range(idx + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of Tweets with important word per PeriodID, based on the EventType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_important_words(tweet):\n",
    "    pattern = r'g+o+a+l+|' \\\n",
    "              r'full\\s+time|' \\\n",
    "              r'half\\s+time|' \\\n",
    "              r'kick\\s+off|' \\\n",
    "              r'owngoal|' \\\n",
    "              r'penalty|' \\\n",
    "              r'red\\s+card|' \\\n",
    "              r'yellow\\s+card|' \\\n",
    "              r'other'\n",
    "    \n",
    "    if re.search(pattern, tweet, re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "important_word_per_period = {}\n",
    "\n",
    "for match_name, df in all_matchs_cleaned.items():\n",
    "    df = df.copy()\n",
    "    df['has_important_word'] = df['Tweet'].apply(has_important_words)\n",
    "\n",
    "    important_word_df = df[df['has_important_word']]\n",
    "\n",
    "    aggregated_data = (\n",
    "        important_word_df.groupby(['PeriodID', 'EventType'])\n",
    "        .size()\n",
    "        .unstack(level=1, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    important_word_per_period[match_name] = aggregated_data\n",
    "\n",
    "num_matches = len(important_word_per_period)\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(num_matches / num_cols)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (match_name, aggregated_data) in enumerate(important_word_per_period.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    aggregated_data['TotalTweets'] = aggregated_data.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "    average = aggregated_data['TotalTweets'].mean()\n",
    "\n",
    "    for _, row in aggregated_data.iterrows():\n",
    "        period_id = row['PeriodID']\n",
    "        retweets_event_1 = row.get(1, 0)\n",
    "        retweets_event_0 = row.get(0, 0)\n",
    "\n",
    "        ax.bar(period_id, retweets_event_1, color='red', label='EventType 1' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "        ax.bar(period_id, retweets_event_0, bottom=retweets_event_1, color='blue', label='EventType 0' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "\n",
    "    ax.axhline(y=average, color='yellow', linestyle='--', linewidth=2, label='Mean')\n",
    "\n",
    "    ax.set_title(f\"Match: {match_name}\", fontsize=10)\n",
    "    ax.set_xlabel(\"PeriodID\", fontsize=8)\n",
    "    ax.set_ylabel(\"Number of tweet with Important Word\", fontsize=8)\n",
    "    ax.grid(True)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), fontsize=6)\n",
    "\n",
    "for j in range(idx + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average tweet length per PeriodID, based on the EventType. (not relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_length(tweet):\n",
    "    return len(tweet.split())\n",
    "\n",
    "tweet_length_per_period = {}\n",
    "\n",
    "for match_name, df in all_matchs_cleaned.items():\n",
    "    df['tweet_length'] = df['Tweet'].apply(tweet_length)\n",
    "\n",
    "    aggregated_data = (\n",
    "        df.groupby(['PeriodID', 'EventType'])['tweet_length']\n",
    "        .mean()\n",
    "        .unstack(level=1, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    tweet_length_per_period[match_name] = aggregated_data\n",
    "\n",
    "num_matches = len(tweet_length_per_period)\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(num_matches / num_cols)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (match_name, aggregated_data) in enumerate(tweet_length_per_period.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for period_id in aggregated_data['PeriodID']:\n",
    "        row = aggregated_data[aggregated_data['PeriodID'] == period_id]\n",
    "        if 1 in row.columns:\n",
    "            avg_length_event_1 = row[1].values[0]\n",
    "        else:\n",
    "            avg_length_event_1 = 0\n",
    "        if 0 in row.columns:\n",
    "            avg_length_event_0 = row[0].values[0]\n",
    "        else:\n",
    "            avg_length_event_0 = 0\n",
    "\n",
    "        ax.bar(period_id, avg_length_event_1, color='red', label='EventType 1' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "        ax.bar(period_id, avg_length_event_0, bottom=avg_length_event_1, color='blue', label='EventType 0' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "\n",
    "    ax.set_title(f\"Match: {match_name}\", fontsize=10)\n",
    "    ax.set_xlabel(\"PeriodID\", fontsize=8)\n",
    "    ax.set_ylabel(\"Average Tweet Length\", fontsize=8)\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=6)\n",
    "\n",
    "for j in range(idx + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of Retweets per PeriodID, based on the EventType (not relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retweet(tweet):\n",
    "    return tweet.startswith('RT @')\n",
    "\n",
    "retweet_per_period = {}\n",
    "\n",
    "for match_name, df in all_matchs.items():\n",
    "    df['is_retweet'] = df['Tweet'].apply(is_retweet)\n",
    "\n",
    "    retweets_df = df[df['is_retweet']]\n",
    "\n",
    "    aggregated_data = (\n",
    "        retweets_df.groupby(['PeriodID', 'EventType'])\n",
    "        .size()\n",
    "        .unstack(level=1, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    retweet_per_period[match_name] = aggregated_data\n",
    "\n",
    "num_matches = len(retweet_per_period)\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(num_matches / num_cols)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (match_name, aggregated_data) in enumerate(retweet_per_period.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for period_id in aggregated_data['PeriodID']:\n",
    "        row = aggregated_data[aggregated_data['PeriodID'] == period_id]\n",
    "        if 1 in row.columns:\n",
    "            retweets_event_1 = row[1].values[0]\n",
    "        else:\n",
    "            retweets_event_1 = 0\n",
    "        if 0 in row.columns:\n",
    "            retweets_event_0 = row[0].values[0]\n",
    "        else:\n",
    "            retweets_event_0 = 0\n",
    "\n",
    "        ax.bar(period_id, retweets_event_1, color='red', label='EventType 1' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "        ax.bar(period_id, retweets_event_0, bottom=retweets_event_1, color='blue', label='EventType 0' if period_id == aggregated_data['PeriodID'].iloc[0] else \"\")\n",
    "\n",
    "    ax.set_title(f\"Match: {match_name}\", fontsize=10)\n",
    "    ax.set_xlabel(\"PeriodID\", fontsize=8)\n",
    "    ax.set_ylabel(\"Number of Retweets\", fontsize=8)\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=6)\n",
    "\n",
    "for j in range(idx + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"glove-twitter-200\"\n",
    "embedding_dim = 200\n",
    "embeddings_model = api.load(embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'rt', '', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "def tokenize_and_lemmatize(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_tweets(tweets):\n",
    "    cleaned = [clean_tweet(tweet) for tweet in tweets]\n",
    "    tokenized = [tokenize_and_lemmatize(tweet) for tweet in cleaned]\n",
    "    return tokenized\n",
    "\n",
    "def get_embedding(tokens, embeddings_model, embedding_dim):\n",
    "    valid_embeddings = [embeddings_model[token] for token in tokens if token in embeddings_model]\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "def create_embeddings(data, embeddings_model, embedding_dim):\n",
    "    embeddings = []\n",
    "    for tweets in data['Tweets']:\n",
    "        tokenized_tweets = preprocess_tweets(tweets)\n",
    "        all_tokens = [token for tweet in tokenized_tweets for token in tweet]\n",
    "        embedding = get_embedding(all_tokens, embeddings_model, embedding_dim)\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_column(df):\n",
    "    df['ID'] = df['MatchID'].astype(str) + '_' + df['PeriodID'].astype(str)\n",
    "    return df\n",
    "\n",
    "def process_match_cleaned(df, is_test=False):\n",
    "    df = add_id_column(df.copy())\n",
    "    \n",
    "    nb_tweets = df.groupby('ID').size().reset_index(name='nb_tweets')\n",
    "    \n",
    "    advanced_patterns = {\n",
    "        \"goal\": [\n",
    "                \"goal\", \"scored\", \"scoring\", \"nets\", \"shot\", \"strikes\", \"finishes\", \"hits\", \"equalizer\", \n",
    "                \"header\", \"top\", \"corner\", \"bottom\", \"worldie\", \"screamer\", \"wonder\", \"amazing\", \"unbelievable\", \n",
    "                \"finish\", \"golazo\", \"back\", \"net\", \"brace\", \"hat\", \"trick\", \"free\", \"kick\", \"open\", \"play\", \n",
    "                \"tapped\", \"in\", \"clinical\", \"last\", \"minute\", \"top\", \"class\", \"strike\", \"blasted\", \"goalie\", \n",
    "                \"keeper\", \"goalkeeper\", \"shoot\", \"target\", \"unstoppable\", \"rebound\", \"assist\", \"attack\", \n",
    "                \"goalbound\", \"counter\", \"offensive\", \"ball\"\n",
    "            ],\n",
    "            \"half_time\": [\n",
    "                \"halftime\", \"half-time\", \"first\", \"half\", \"break\", \"intermission\", \"end\", \"ht\", \"whistle\", \n",
    "                \"time\", \"rest\", \"half\", \"stats\", \"rest\", \"period\", \"interval\", \"cooling\", \"pause\", \"review\", \n",
    "                \"recap\", \"highlights\", \"analysis\", \"midway\", \"stop\", \"halfway\", \"breakdown\", \"mid\", \"review\",\n",
    "            ],\n",
    "            \"kick_off\": [\n",
    "                \"kickoff\", \"kick-off\", \"start\", \"game\", \"on\", \"match\", \"started\", \"blown\", \"underway\", \n",
    "                \"begins\", \"starting\", \"early\", \"minutes\", \"live\", \"first\", \"opening\", \"whistle\", \"action\", \n",
    "                \"beginning\", \"intro\", \"debut\", \"matchday\", \"clash\", \"duel\", \"showdown\", \"kick\",\n",
    "            ],\n",
    "            \"full_time\": [\n",
    "                \"fulltime\", \"full-time\", \"end\", \"final\", \"finished\", \"over\", \"match\", \"ends\", \"score\", \n",
    "                \"game\", \"result\", \"wraps\", \"ending\", \"finish\", \"summary\", \"conclusion\", \"closure\", \n",
    "                \"last\", \"blown\", \"finalized\", \"full\", \"stoppage\", \"post\", \"reaction\", \"outcome\", \"finalized\",\n",
    "            ],\n",
    "            \"penalty\": [\n",
    "                \"penalty\", \"spot\", \"kick\", \"pk\", \"pen\", \"awarded\", \"given\", \"shoots\", \"saved\", \n",
    "                \"converts\", \"miss\", \"scored\", \"shootout\", \"decider\", \"call\", \"shot\", \"var\", \"decision\", \n",
    "                \"penalized\", \"keeper\", \"goalkeeper\", \"foul\", \"penalties\", \"shoot\", \"retake\", \"blocked\", \n",
    "                \"controversy\", \"chance\", \"conversion\"\n",
    "            ],\n",
    "            \"red_card\": [\n",
    "                \"red\", \"card\", \"sent\", \"off\", \"ejected\", \"foul\", \"dangerous\", \"tackle\", \"reckless\", \n",
    "                \"violent\", \"conduct\", \"second\", \"straight\", \"dismissed\", \"player\", \"bad\", \n",
    "                \"issued\", \"poor\", \"team\", \"ten\", \"man\", \"short\", \"disciplinary\", \"harsh\", \"controversial\", \n",
    "                \"deserved\", \"unfair\", \"redcard\", \"offense\", \"foulplay\", \"referee\", \"decision\", \"call\",\n",
    "            ],\n",
    "            \"yellow_card\": [\n",
    "                \"yellow\", \"card\", \"booked\", \"caution\", \"warning\", \"reckless\", \"tackle\", \"late\", \n",
    "                \"challenge\", \"rough\", \"play\", \"soft\", \"minor\", \"team\", \"warned\", \"fouled\", \"cheap\", \n",
    "                \"booking\", \"sloppy\", \"physical\", \"yellowcard\", \"persistence\", \"persistent\", \"referee\",\n",
    "            ],\n",
    "            \"own_goal\": [\n",
    "                \"own\", \"goal\", \"og\", \"unlucky\", \"deflected\", \"mistake\", \"blunder\", \"misjudged\", \n",
    "                \"wrong\", \"net\", \"disaster\", \"horror\", \"show\", \"scored\", \"self\", \"tragic\", \"moment\", \n",
    "                \"oops\", \"misplay\", \"accidental\", \"misstep\", \"owngoal\", \"oopsie\", \"calamity\",\n",
    "            ],\n",
    "            \"win\": [\n",
    "                \"win\", \"victory\", \"triumph\", \"beat\", \"beats\", \"defeats\", \"wins\", \"victorious\", \n",
    "                \"champions\", \"conquered\", \"game\", \"match\", \"big\", \"close\", \"amazing\", \"sealed\", \n",
    "                \"final\", \"team\", \"domination\", \"celebration\", \"winning\", \"champion\", \"success\", \n",
    "                \"through\", \"qualified\", \"advanced\"\n",
    "            ],\n",
    "            \"loss\": [\n",
    "                \"loss\", \"defeated\", \"lose\", \"beaten\", \"eliminated\", \"lost\", \"downfall\", \"knocked\", \n",
    "                \"out\", \"upset\", \"heartbreak\", \"poor\", \"final\", \"last\", \"chance\", \"missed\", \"opportunity\", \n",
    "                \"team\", \"falls\", \"failure\", \"fallen\", \"blown\", \"defeat\", \"crushed\", \"disappointed\", \n",
    "                \"despair\", \"heartbreaking\", \"disappointment\", \"failure\", \"elimination\", \"out\", \"crash\",\n",
    "            ],\n",
    "            \"other_event\": [\n",
    "                \"match\", \"game\", \"whistle\", \"score\", \"var\", \"referee\", \"foul\", \"play\", \"team\", \n",
    "                \"offside\", \"substitution\", \"injury\", \"celebration\", \"comeback\", \"drama\", \"crowd\", \n",
    "                \"roar\", \"pitch\", \"stadium\", \"matchday\", \"football\", \"fans\", \"spirit\", \"final\", \n",
    "                \"minutes\", \"second\", \"extra\", \"time\", \"stoppage\", \"preview\", \"post\", \"pre\", \"live\", \n",
    "                \"performance\", \"form\", \"support\", \"home\", \"away\", \"underdog\", \"giant\", \"clash\", \n",
    "                \"fixture\", \"tournament\", \"season\", \"points\", \"table\", \"record\", \"momentum\", \"suspense\"\n",
    "            ]\n",
    "    }\n",
    "    \n",
    "    def construct_pattern(word):\n",
    "        pattern_chars = []\n",
    "        for c in word:\n",
    "            if c == ' ' or c == '-':\n",
    "                pattern_chars.append(r'\\s*')\n",
    "            else:\n",
    "                pattern_chars.append(f'{re.escape(c)}+')\n",
    "        return ''.join(pattern_chars)\n",
    "    \n",
    "    compiled_patterns = {}\n",
    "    for key, words in advanced_patterns.items():\n",
    "        regex_patterns = [construct_pattern(word) for word in words]\n",
    "        combined_pattern = r'|'.join(regex_patterns)\n",
    "        compiled_patterns[key] = combined_pattern\n",
    "\n",
    "    for event, pattern in compiled_patterns.items():\n",
    "        df[f'has_{event}'] = df['Tweet'].str.contains(pattern, flags=re.IGNORECASE, regex=True)\n",
    "    \n",
    "    event_columns = [f'has_{event}' for event in compiled_patterns.keys()]\n",
    "    event_mentions = df.groupby('ID')[event_columns].sum().reset_index()\n",
    "    \n",
    "    result = pd.merge(event_mentions, nb_tweets, on='ID')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(all_matchs_cleaned, is_test=False):\n",
    "    all_features_cleaned = []\n",
    "    for match_id, df in all_matchs_cleaned.items():\n",
    "        features = process_match_cleaned(df, is_test)\n",
    "        all_features_cleaned.append(features)\n",
    "    all_features_cleaned_df = pd.concat(all_features_cleaned, ignore_index=True)\n",
    "\n",
    "    def sort_ids(df):\n",
    "        df['MatchID_sort'] = df['ID'].str.split('_').str[0].astype(int)\n",
    "        df['PeriodID_sort'] = df['ID'].str.split('_').str[1].astype(int)\n",
    "        df = df.sort_values(by=['MatchID_sort', 'PeriodID_sort'])\n",
    "        df = df.drop(columns=['MatchID_sort', 'PeriodID_sort'])\n",
    "        return df\n",
    "\n",
    "    final_df = sort_ids(all_features_cleaned_df).reset_index(drop=True)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for match, df in all_matchs_cleaned.items():\n",
    "    for id, group in df.groupby('ID'):\n",
    "        tweets = group['Tweet'].tolist()\n",
    "        event_type = group['EventType'].iloc[0]\n",
    "        data_list.append({\n",
    "            'ID': id,\n",
    "            'EventType': event_type,\n",
    "            'Tweets': tweets\n",
    "        })\n",
    "\n",
    "data = pd.DataFrame(data_list)\n",
    "data[['MatchID', 'PeriodID']] = data['ID'].str.split('_', expand=True)\n",
    "data['MatchID'] = data['MatchID'].astype(int)\n",
    "data['PeriodID'] = data['PeriodID'].astype(int)\n",
    "data = data.sort_values(by=['MatchID', 'PeriodID']).reset_index(drop=True)\n",
    "data = data.drop(columns=['MatchID', 'PeriodID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_df = create_new_features(all_matchs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "final_df_scaled = new_features_df.copy()\n",
    "columns_to_scale = new_features_df.columns.difference(['ID'])\n",
    "final_df_scaled[columns_to_scale] = scaler.fit_transform(new_features_df[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.merge(data, final_df_scaled, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings = create_embeddings(data, embeddings_model, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = data['EventType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Embeddings.npy', X)\n",
    "# np.save('Labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.load('Embeddings.npy')\n",
    "# y = np.load('Labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features = [col for col in final_data.columns if col.startswith('has_')] + ['nb_tweets']\n",
    "X_additional = final_data[additional_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X_embeddings, X_additional))\n",
    "y = final_data['EventType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_tsne[y == 0, 0], X_tsne[y == 0, 1], c='blue', label='Class 0', alpha=0.5)\n",
    "plt.scatter(X_tsne[y == 1, 0], X_tsne[y == 1, 1], c='red', label='Class 1', alpha=0.5)\n",
    "plt.title('t-SNE 2D Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntraînement du modèle Logistic Regression...\")\n",
    "clf = LogisticRegression(max_iter=439, C = 2.1944043721683357, penalty = 'l2', solver = 'liblinear', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_lr = clf.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f'Accuracy Logistic Regression: {accuracy_lr:.4f}')\n",
    "print('Classification Report Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_lr = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': uniform(0.01, 10),\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': randint(100, 1000)\n",
    "}\n",
    "\n",
    "random_search_lr = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_distributions=param_dist_lr,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_lr.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", random_search_lr.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", random_search_lr.best_score_)\n",
    "\n",
    "best_lr = random_search_lr.best_estimator_\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f'Accuracy Logistic Regression: {accuracy_lr:.4f}')\n",
    "print('Classification Report Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntraînement du modèle Random Forest...\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Accuracy Random Forest: {accuracy_rf:.4f}')\n",
    "print('Classification Report Random Forest:')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Meilleurs paramètres : {'bootstrap': True, 'max_depth': 95, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1247}\n",
    "# Meilleure précision (Cross-validation): 0.7765886287625419\n",
    "# Accuracy Random Forest: 0.7336\n",
    "# Classification Report Random Forest:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.73      0.67      0.70       296\n",
    "#            1       0.74      0.79      0.76       346\n",
    "\n",
    "#     accuracy                           0.73       642\n",
    "#    macro avg       0.73      0.73      0.73       642\n",
    "# weighted avg       0.73      0.73      0.73       642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_rf = {\n",
    "    'n_estimators': randint(100, 2000),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 100),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", random_search_rf.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", random_search_rf.best_score_)\n",
    "\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Accuracy Random Forest: {accuracy_rf:.4f}')\n",
    "print('Classification Report Random Forest:')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntraînement du modèle Linéaire SVM...\")\n",
    "svm_clf = LinearSVC(random_state=42, C = 100, loss = 'hinge', max_iter = 3000, tol = 0.0001)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'Accuracy SVM: {accuracy_svm:.4f}')\n",
    "print('Classification Report Linéaire SVM:')\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "param_dist_svc = {\n",
    "    'C': uniform(0.01, 10),\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'max_iter': randint(1000, 5000),\n",
    "    'tol': uniform(1e-5, 1e-1)\n",
    "}\n",
    "\n",
    "random_search_svc = RandomizedSearchCV(\n",
    "    estimator=LinearSVC(random_state=42),\n",
    "    param_distributions=param_dist_svc,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_svc.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", random_search_svc.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", random_search_svc.best_score_)\n",
    "\n",
    "best_svc = random_search_svc.best_estimator_\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(f'Accuracy Linear SVC: {accuracy_svc:.4f}')\n",
    "print('Classification Report Linear SVC:')\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntraînement du modèle non linéaire SVM...\")\n",
    "svm_rbf = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_svm_rbf = svm_rbf.predict(X_test)\n",
    "accuracy_svm_rbf = accuracy_score(y_test, y_pred_svm_rbf)\n",
    "print(f'Accuracy SVM: {accuracy_svm_rbf:.4f}')\n",
    "print('Classification Report Linéaire SVM:')\n",
    "print(classification_report(y_test, y_pred_svm_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "param_dist_svc = {\n",
    "    'C': uniform(0.01, 10),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': randint(2, 5),\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': uniform(0, 1),\n",
    "    'tol': uniform(1e-5, 1e-1),\n",
    "    'max_iter': randint(1000, 5000)\n",
    "}\n",
    "\n",
    "random_search_svc = G(\n",
    "    estimator=SVC(random_state=42),\n",
    "    param_distributions=param_dist_svc,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_svc.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", random_search_svc.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", random_search_svc.best_score_)\n",
    "\n",
    "best_svc = random_search_svc.best_estimator_\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(f'Accuracy SVC: {accuracy_svc:.4f}')\n",
    "print('Classification Report SVC:')\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_mlp = {\n",
    "    'hidden_layer_sizes': [(10,), (10,10), (50,10), (100,10), (50, 50), (100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.001, 0.004, 0.002, 0.003],\n",
    "    'learning_rate': ['constant'],\n",
    "    'max_iter': [500, 750, 1000],\n",
    "    'batch_size': [32, 64, 128],\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=42),\n",
    "    param_grid=param_dist_mlp,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_mlp.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", grid_search_mlp.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", grid_search_mlp.best_score_)\n",
    "\n",
    "best_mlp = grid_search_mlp.best_estimator_\n",
    "y_pred_mlp = best_mlp.predict(X_test)\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f'Accuracy MLP: {accuracy_mlp:.4f}')\n",
    "print('Classification Report MLP:')\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# random_search_mlp = RandomizedSearchCV(\n",
    "#     estimator=MLPClassifier(random_state=42),\n",
    "#     param_distributions=param_dist_mlp,\n",
    "#     n_iter=100,\n",
    "#     scoring='accuracy',\n",
    "#     verbose=2,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# random_search_mlp.fit(X_train, y_train)\n",
    "# print(\"Meilleurs paramètres :\", random_search_mlp.best_params_)\n",
    "# print(\"Meilleure précision (Cross-validation):\", random_search_mlp.best_score_)\n",
    "\n",
    "# best_mlp = random_search_mlp.best_estimator_\n",
    "# y_pred_mlp = best_mlp.predict(X_test)\n",
    "# accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "# print(f'Accuracy MLP: {accuracy_mlp:.4f}')\n",
    "# print('Classification Report MLP:')\n",
    "# print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 10), \n",
    "    activation='relu', \n",
    "    solver='adam', \n",
    "    alpha=0.003, \n",
    "    learning_rate='constant', \n",
    "    batch_size=64,\n",
    "    max_iter=980, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f'Accuracy MLP: {accuracy_mlp:.4f}')\n",
    "print('Classification Report MLP:')\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# Meilleurs paramètres : {'activation': 'logistic', 'alpha': 0.003132655146732228, 'batch_size': 122, 'hidden_layer_sizes': (10,), 'learning_rate': 'constant', 'max_iter': 980, 'solver': 'adam'}\n",
    "# Meilleure précision (Cross-validation): 0.7498327759197324\n",
    "# Accuracy MLP: 0.7445\n",
    "# Classification Report MLP:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.74      0.69      0.71       296\n",
    "#            1       0.75      0.79      0.77       346\n",
    "\n",
    "#     accuracy                           0.74       642\n",
    "#    macro avg       0.74      0.74      0.74       642\n",
    "# weighted avg       0.74      0.74      0.74       642\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 1),\n",
    "    'max_depth': randint(3, 30),\n",
    "    'n_estimators': randint(1000, 10000),\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres :\", random_search.best_params_)\n",
    "print(\"Meilleure précision (Cross-validation):\", random_search.best_score_)\n",
    "\n",
    "best_xgb_clf = random_search.best_estimator_\n",
    "y_pred_xgb = best_xgb_clf.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy XGBoost: {accuracy_xgb:.4f}')\n",
    "print('Classification Report XGBoost:')\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mac\n",
    "# Meilleurs paramètres : {'colsample_bytree': 0.5110923710151508, 'gamma': 0.49816518664589293, 'learning_rate': 0.15286320903670425, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 5363, 'subsample': 0.9081928676065312}\n",
    "# Meilleure précision (Cross-validation): 0.7872909698996656\n",
    "# Accuracy XGBoost: 0.7523\n",
    "# Classification Report XGBoost:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.72      0.76      0.74       296\n",
    "#            1       0.79      0.74      0.76       346\n",
    "\n",
    "#     accuracy                           0.75       642\n",
    "#    macro avg       0.75      0.75      0.75       642\n",
    "# weighted avg       0.75      0.75      0.75       642\n",
    "\n",
    "#poly :\n",
    "# Meilleurs paramètres : {'colsample_bytree': 0.6289514135224699, 'gamma': 0.17088758739006582, 'learning_rate': 0.2105929659773293, 'max_depth': 27, 'min_child_weight': 2, 'n_estimators': 1846, 'subsample': 0.7675183660224553}\n",
    "# Meilleure précision (Cross-validation): 0.7832775919732442\n",
    "# Accuracy XGBoost: 0.7477\n",
    "# Classification Report XGBoost:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.72      0.74      0.73       296\n",
    "#            1       0.77      0.76      0.76       346\n",
    "\n",
    "#     accuracy                           0.75       642\n",
    "#    macro avg       0.75      0.75      0.75       642\n",
    "# weighted avg       0.75      0.75      0.75       642\n",
    "\n",
    "#anciens parametres probablement 0.7\n",
    "# xgb_clf = XGBClassifier(\n",
    "#     n_estimators=1000,          # Nombre d'arbres\n",
    "#     max_depth=10,               # Profondeur maximale des arbres\n",
    "#     learning_rate=0.1,         # Taux d'apprentissage\n",
    "#     subsample=0.8,             # Fraction des échantillons à utiliser pour chaque arbre\n",
    "#     colsample_bytree=0.8,      # Fraction des caractéristiques à utiliser pour chaque arbre\n",
    "#     objective='binary:logistic',  # Objectif pour classification binaire\n",
    "#     use_label_encoder=False,   # Désactiver l'encodeur de labels (pour éviter les avertissements)\n",
    "#     eval_metric='logloss',     # Métrique d'évaluation\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# Meilleurs paramètres : {'colsample_bytree': 0.9090073829612466, 'gamma': 0.8607305832563434, 'learning_rate': 0.01208563915935721, 'max_depth': 10, 'min_child_weight': 9, 'n_estimators': 5413, 'subsample': 0.6110539052353652}\n",
    "# Meilleure précision (Cross-validation): 0.765886287625418\n",
    "# Accuracy XGBoost: 0.7430\n",
    "# Classification Report XGBoost:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.72      0.73      0.72       296\n",
    "#            1       0.76      0.76      0.76       346\n",
    "\n",
    "#     accuracy                           0.74       642\n",
    "#    macro avg       0.74      0.74      0.74       642\n",
    "# weighted avg       0.74      0.74      0.74       642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=5000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    eta=0.01,\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy XGBoost: {accuracy_xgb:.4f}')\n",
    "print('Classification Report XGBoost:')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# xgb_clf = XGBClassifier(\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=6,\n",
    "#     learning_rate=0.01,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='logloss',\n",
    "#     random_state=42,\n",
    "#     reg_lambda=10,\n",
    "#     reg_alpha=10,\n",
    "#     min_child_weight=30,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = xgb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_list = []\n",
    "for match, df in all_matchs_cleaned_eval.items():\n",
    "    for id, group in df.groupby('ID'):\n",
    "        tweets = group['Tweet'].tolist()\n",
    "        data_eval_list.append({\n",
    "            'ID': id,\n",
    "            'Tweets': tweets\n",
    "        })\n",
    "\n",
    "data_eval = pd.DataFrame(data_eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_eval_df = create_new_features(all_matchs_cleaned_eval, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_df_scaled = new_features_eval_df.copy()\n",
    "columns_to_scale_eval = new_features_eval_df.columns.difference(['ID'])\n",
    "final_eval_df_scaled[columns_to_scale_eval] = scaler.fit_transform(new_features_eval_df[columns_to_scale_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_eval = pd.merge(data_eval, final_eval_df_scaled, on=['ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Création des embeddings pour les données d'évaluation...\")\n",
    "X_eval_embeddings = create_embeddings(data_eval, embeddings_model, embedding_dim)\n",
    "print(\"Création des embeddings terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Embeddings_eval.npy', X_eval_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_eval_embeddings = np.load('Embeddings_eval.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features_eval = [col for col in final_data.columns if col.startswith('has_')] + ['nb_tweets']\n",
    "X_eval_additional = final_data_eval[additional_features_eval].values\n",
    "X_eval = np.hstack((X_eval_embeddings, X_eval_additional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval = chosen_model.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'ID': data_eval['ID'],\n",
    "    'EventType': y_pred_eval\n",
    "})\n",
    "\n",
    "submission[['num1', 'num2']] = submission['ID'].str.split('_', expand=True).astype(int)\n",
    "submission = submission.sort_values(by=['num1', 'num2']).drop(columns=['num1', 'num2'])\n",
    "\n",
    "submission.to_csv(f'submission_XGB_scaled_ha.csv', index=False)\n",
    "print(\"Les prédictions ont été sauvegardées dans 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = chosen_model.predict_proba(X)\n",
    "y_eval_proba = chosen_model.predict_proba(X_eval_embeddings)\n",
    "\n",
    "submission_proba_y_pred = pd.DataFrame({\n",
    "    'ID': data['ID'],\n",
    "    'xgb_proba_0': y_pred_proba[:, 0],\n",
    "    'xgb_proba_1': y_pred_proba[:, 1]\n",
    "})\n",
    "\n",
    "submission_proba_y_eval = pd.DataFrame({\n",
    "    'ID': data_eval['ID'],\n",
    "    'xgb_proba_0': y_eval_proba[:, 0],\n",
    "    'xgb_proba_1': y_eval_proba[:, 1]\n",
    "})\n",
    "\n",
    "submission_proba = pd.concat([submission_proba_y_pred, submission_proba_y_eval], ignore_index=True)\n",
    "submission_proba[['num1', 'num2']] = submission_proba['ID'].str.split('_', expand=True).astype(int)\n",
    "submission_proba = submission_proba.sort_values(by=['num1', 'num2']).drop(columns=['num1', 'num2'])\n",
    "\n",
    "submission_proba.to_csv(f'submission_proba_XGB_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
